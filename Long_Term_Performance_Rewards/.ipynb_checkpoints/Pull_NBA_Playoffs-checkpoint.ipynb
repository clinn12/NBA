{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull NBA Playoffs Data \n",
    "##### Webscrapper to pull NBA Playoffs data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input to specify the year processing will begin\n",
    "start_year = 1960\n",
    "\n",
    "#Input to specify the file to load and save updated output\n",
    "file_path = 'nba_playoffs.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from datetime import datetime\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup, Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Functions to Scrape and Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_playoff_year():\n",
    "    \"\"\"\n",
    "    Determine the current NBA playoff year and whether it should be processed.\n",
    "\n",
    "    NBA playoffs run from April through June.\n",
    "    - If today's date is in June or before, the playoff have not happened yet.\n",
    "    - Processing is skipped for the current season if the playoffs are not completed.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - year (int): The NBA season year (e.g., 2025).\n",
    "    \"\"\"\n",
    "    \n",
    "    #Identify Date\n",
    "    today = datetime.today()\n",
    "    \n",
    "    #Identify Year and Season\n",
    "    if today.month < 5:  #Playoffs don't start until late April\n",
    "        year = today.year - 1\n",
    "    else:\n",
    "        year = today.year\n",
    "    \n",
    "    #Return Objects\n",
    "    return year\n",
    "\n",
    "\n",
    "def load_existing_data(file_path, min_size_kb):\n",
    "    \"\"\"\n",
    "    Load existing NBA playoff data from CSV if it exists and is large enough.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing standings.\n",
    "        min_size_kb (int, optional): Minimum file size in kilobytes to consider\n",
    "                                     the file valid. Defaults to 1 KB.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - DataFrame: Existing standings data, or an empty DataFrame if not found/invalid.\n",
    "            - set[int]: Unique season years already present in the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Identify File\n",
    "    if os.path.exists(file_path) and os.path.getsize(file_path) > (min_size_kb * 1024):\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df, set(df[\"Year\"].unique())\n",
    "    \n",
    "    #Return Dataframe\n",
    "    return pd.DataFrame(), set()\n",
    "\n",
    "\n",
    "def fetch_playoff_html(year):\n",
    "    \"\"\"\n",
    "    Fetch the NBA playoff HTML page for a given season year.\n",
    "\n",
    "    Args:\n",
    "        year (int): The NBA season year (e.g., 2025 for the 2024–25 season).\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup: Parsed HTML content of the playoff page.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Scrape Basketball Reference\n",
    "    url = f\"https://www.basketball-reference.com/playoffs/NBA_{year}_standings.html\"\n",
    "    html = urlopen(url)\n",
    "\n",
    "    #Return Results\n",
    "    return BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "\n",
    "def parse_playoff(soup, year):\n",
    "    \"\"\"\n",
    "    Parse a playoff HTML page into a cleaned pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML of the playoff page.\n",
    "        year (int): The NBA season year.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned standings data with columns\n",
    "    \"\"\"\n",
    "    \n",
    "    #Find all HTML comments\n",
    "    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "\n",
    "    #Create an Object that will be in our For Loop to identify once we find the first table\n",
    "    first_table_found = False\n",
    "\n",
    "    #Parse and find the playoff table needed\n",
    "    for comment in comments:\n",
    "        if '<table' in comment and not first_table_found:\n",
    "            table_soup = BeautifulSoup(comment, 'html.parser')\n",
    "            table = table_soup.find('table')\n",
    "\n",
    "            if table:\n",
    "                first_table_found = True  #Mark as found so we stop after this\n",
    "\n",
    "                #Get headers (second <tr>)\n",
    "                header_row = table.find_all('tr')[1]\n",
    "                headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n",
    "\n",
    "                #Get all data rows starting after the header row\n",
    "                rows = []\n",
    "                for tr in table.find_all('tr')[2:]:\n",
    "                    cells = [td.get_text(strip=True) for td in tr.find_all(['th', 'td'])]\n",
    "                    if cells:  #Skip empty rows\n",
    "                        rows.append(cells)\n",
    "\n",
    "            break  #Exit after first table\n",
    "\n",
    "    #Create Dataframe and Rename Columns\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "    \n",
    "    #Create Year\n",
    "    df['Year'] = year\n",
    "\n",
    "    #Create Wins\n",
    "    df['Wins'] = df['Overall'].str[:2]\n",
    "    invalidchar = string.punctuation\n",
    "    df['Wins'] = pd.to_numeric(df['Wins'].str.strip(invalidchar))\n",
    "\n",
    "    #Identify Champion and Conference Champion by observing Win totals\n",
    "    #The playoffs were restructed in 1984 to include 16 teams and no bye teams\n",
    "    if year <= 1983:\n",
    "        df['E_Wins'] = df['E'].str[:2]\n",
    "        df['E_Losses'] = df['E'].str[-2:]\n",
    "        df['W_Wins'] = df['W'].str[:2]\n",
    "        df['W_Losses'] = df['W'].str[-2:]\n",
    "        df['E_Wins'] = df['E_Wins'].str.strip(invalidchar)\n",
    "        df['E_Losses'] = df['E_Losses'].str.strip(invalidchar)\n",
    "        df['W_Wins'] = df['W_Wins'].str.strip(invalidchar)\n",
    "        df['W_Losses'] = df['W_Losses'].str.strip(invalidchar)\n",
    "        df['E_Wins'] = pd.to_numeric(df['E_Wins'])\n",
    "        df['E_Losses'] = pd.to_numeric(df['E_Losses'])\n",
    "        df['W_Wins'] = pd.to_numeric(df['W_Wins'])\n",
    "        df['W_Losses'] = pd.to_numeric(df['W_Losses'])\n",
    "        df['E_Games'] = df['E_Wins'] + df['E_Losses']\n",
    "        df['W_Games'] = df['W_Wins'] + df['W_Losses']\n",
    "        df['Conference_Champion'] = np.where((df['E_Games'] >=1) & (df['W_Games'] >=1), 1, 0)\n",
    "        df['Champion'] = np.where((df['E_Wins'] >=4) & (df['W_Wins'] >=4), 1, 0)\n",
    "    else:\n",
    "        df['Wins'] = df['Overall'].str[:2]\n",
    "        df['Wins'] = pd.to_numeric(df['Wins'].str.strip(invalidchar))\n",
    "        max_wins = df['Wins'].max()\n",
    "        second_max_wins = df['Wins'].nlargest(2).iloc[1]\n",
    "        df['Champion'] = np.where(df['Wins'] >= max_wins, 1, 0)\n",
    "        df['Conference_Champion'] = np.where(df['Wins'] >= second_max_wins, 1, 0)\n",
    "\n",
    "    #Return Dataframe\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main script entry point for scraping and updating NBA playoff data.\n",
    "\n",
    "    Workflow:\n",
    "        1. Determine the current season year and processing eligibility.\n",
    "        2. Load existing data (if available).\n",
    "        3. Loop through season years from start_year to current_season_year.\n",
    "        4. Skip already-processed years (except eligible current season).\n",
    "        5. Fetch and parse standings HTML.\n",
    "        6. Append new data and save the updated CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    #Run Function to Identify Years to Process\n",
    "    current_season_year = get_current_playoff_year()\n",
    "    \n",
    "    #Run Function to Load Existing Data\n",
    "    existing_df, existing_years = load_existing_data(file_path, min_size_kb=1)\n",
    "\n",
    "    #Begin Scraping each Year\n",
    "    for year in range(start_year, current_season_year +1):\n",
    "        if year in existing_years:\n",
    "            print(f\"Skipping {year} — already processed or not in season.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {year}...\")\n",
    "        soup = fetch_playoff_html(year)\n",
    "        playoff_df = parse_playoff(soup, year)\n",
    "        existing_df = pd.concat([existing_df, playoff_df], ignore_index=True)\n",
    "        existing_df = existing_df.sort_values(by=['Year','Champion','Conference_Champion','Wins'], ascending=[False, False, False, False])\n",
    "    \n",
    "    #Update the Standings File\n",
    "    existing_df.to_csv(file_path, index=False)\n",
    "    print(\"Updated nba_playoffs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1960...\n",
      "Processing 1961...\n",
      "Processing 1962...\n",
      "Processing 1963...\n",
      "Processing 1964...\n",
      "Processing 1965...\n",
      "Processing 1966...\n",
      "Processing 1967...\n",
      "Processing 1968...\n",
      "Processing 1969...\n",
      "Processing 1970...\n",
      "Processing 1971...\n",
      "Processing 1972...\n",
      "Processing 1973...\n",
      "Processing 1974...\n",
      "Processing 1975...\n",
      "Processing 1976...\n",
      "Processing 1977...\n",
      "Processing 1978...\n",
      "Processing 1979...\n",
      "Processing 1980...\n",
      "Processing 1981...\n",
      "Processing 1982...\n",
      "Skipping 1983 — already processed or not in season.\n",
      "Skipping 1984 — already processed or not in season.\n",
      "Skipping 1985 — already processed or not in season.\n",
      "Skipping 1986 — already processed or not in season.\n",
      "Skipping 1987 — already processed or not in season.\n",
      "Skipping 1988 — already processed or not in season.\n",
      "Skipping 1989 — already processed or not in season.\n",
      "Skipping 1990 — already processed or not in season.\n",
      "Skipping 1991 — already processed or not in season.\n",
      "Skipping 1992 — already processed or not in season.\n",
      "Skipping 1993 — already processed or not in season.\n",
      "Skipping 1994 — already processed or not in season.\n",
      "Skipping 1995 — already processed or not in season.\n",
      "Skipping 1996 — already processed or not in season.\n",
      "Skipping 1997 — already processed or not in season.\n",
      "Skipping 1998 — already processed or not in season.\n",
      "Skipping 1999 — already processed or not in season.\n",
      "Skipping 2000 — already processed or not in season.\n",
      "Skipping 2001 — already processed or not in season.\n",
      "Skipping 2002 — already processed or not in season.\n",
      "Skipping 2003 — already processed or not in season.\n",
      "Skipping 2004 — already processed or not in season.\n",
      "Skipping 2005 — already processed or not in season.\n",
      "Skipping 2006 — already processed or not in season.\n",
      "Skipping 2007 — already processed or not in season.\n",
      "Skipping 2008 — already processed or not in season.\n",
      "Skipping 2009 — already processed or not in season.\n",
      "Skipping 2010 — already processed or not in season.\n",
      "Skipping 2011 — already processed or not in season.\n",
      "Skipping 2012 — already processed or not in season.\n",
      "Skipping 2013 — already processed or not in season.\n",
      "Skipping 2014 — already processed or not in season.\n",
      "Skipping 2015 — already processed or not in season.\n",
      "Skipping 2016 — already processed or not in season.\n",
      "Skipping 2017 — already processed or not in season.\n",
      "Skipping 2018 — already processed or not in season.\n",
      "Skipping 2019 — already processed or not in season.\n",
      "Skipping 2020 — already processed or not in season.\n",
      "Skipping 2021 — already processed or not in season.\n",
      "Skipping 2022 — already processed or not in season.\n",
      "Skipping 2023 — already processed or not in season.\n",
      "Skipping 2024 — already processed or not in season.\n",
      "Skipping 2025 — already processed or not in season.\n",
      "Updated nba_playoffs.csv\n"
     ]
    }
   ],
   "source": [
    "#Run the main function to scrape standings\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
